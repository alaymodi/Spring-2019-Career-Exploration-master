{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Bias-Variance Tradeoff, Regularization\n",
    "## 4/1/19\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Authored by [Ajay Raj](mailto:araj@berkeley.edu), [Nichole Sun](mailto:nicholesun@berkeley.edu), [Rosa Choe](mailto:rosachoe@berkeley.edu), [Calvin Chen](mailto:chencalvin99@berkeley.edu), and [Roland Chin](mailto:rond24933chn@berkeley.edu).\n",
    "\n",
    "### Table Of Contents\n",
    "* [Recap](#recap)\n",
    "* [Bias-Variance Tradeoff](#bv-tradeoff)\n",
    "    * [Bias](#bias)\n",
    "    * [Variance](#variance)\n",
    "    * [The Tradeoff](#the-tradeoff)\n",
    "    * [Polynomial Regression](#polynomial-regression)\n",
    "* [Regularization](#regularization)\n",
    "    * [Ridge](#ridge)\n",
    "    * [LASSO](#lasso)\n",
    "    * [Visualizing Ridge and Lasso](#visualizing-ridge-and-lasso)\n",
    "    * [Regularization and Bias Variance](#regularization-and-bias-variance)\n",
    "    * [Lambda](#lambda)\n",
    "    * [Validation on Lambda](#validation-on-lambda)\n",
    "* [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotting import overfittingDemo, plot_multiple_linear_regression, ridgeRegularizationDemo, lassoRegularizationDemo\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recap'></a>\n",
    "# Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](fit_graphs.png \"Fit Graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High bias corresponds to underfitting. If we look at the first model, the points seem to follow some sort of curve, but our predictor is linear and therefore, unable to capture all the points. In this case, we have chosen a model which is not complex enough to accurately capture all the information from our data set. \n",
    "\n",
    "If we look at the last model, the predictor is now overly complex because it adjusts based on every point in order to get as close to every data point as possible. In this case, the model changes too much based on small fluctuations caused by insignificant details in the data. This model is fitting more to noise than signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bv-tradeoff'></a>\n",
    "# Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll perform **model evaluation**, where we'll judge how our linear regression models actually perform. Last week, we talked about **loss functions**, which describes a numerical value for how far your model is from the true values.\n",
    "\n",
    "$$\\text{Mean Squared Error: } \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2$$\n",
    "\n",
    "In this loss function, $y_i$ is a scalar, and $x_i$ is a $p$ dimensional vector, because there are $p$ features. This loss is called **mean squared error**, or **MSE**.\n",
    "\n",
    "Now, we'll talk about other ways to evaluate a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, let's define some terms.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that everything in the universe can be described with the following equation:\n",
    "\n",
    "$$y = h(x) + \\epsilon$$\n",
    "\n",
    "- $y$ is the quantity you are trying to model or approximate\n",
    "- $x$ are the features (independent variables)\n",
    "- $h$ is the **true model** for $y$ in terms of $x$\n",
    "- $\\epsilon$ represents **noise**, a random variable which has mean zero\n",
    "\n",
    "Let $f$ be your model for $y$ in terms of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<a id='bias'></a>\n",
    "## Bias\n",
    "\n",
    "When evaluating a model, the most intuitive first step is to look at how well the model performs. For classification, this may be the percentage of data points correctly classified, or for regression it may be how close the predicted values are to actual. The **bias** of a model is *a measure of how close our prediction is to the actual value on average from an average model*. \n",
    "\n",
    "Note that bias is **not** a measure of a single model, it encapuslates the scenario in which we collect many datasets, create models for each dataset, and average the error over all of models. Bias is not a measure of error for a single model, but a more abstract concept describing the average error over all errors. A low value for the bias of a model describes that on average, our predictions are similar to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='variance'></a>\n",
    "## Variance\n",
    "The **variance** of  a model relates to the variance of the distribution of all models. In the previous section about bias, we envisoned the scenario of collecting many datasets, creating models for each dataset, and averaging the error overall the datasets. Instead, the variance of a model describes the variance in prediction. While we might be able to predict a value very well on average, if the variance of predictions is very high this may not be very helpful, as when we train a model we only have one such instance, and a high model variance tells us little about the true nature of the predictions. A low variance describes that our model will not predict very different values for different datasets.\n",
    "\n",
    "**We can take a look at how bias and variance differ and can be explained in a dataset with the following diagram:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](BiasVariance.jpg \"Bias Variance Visualization\")\n",
    "Image from http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image describes what bias and variance are in a more simplified example. Consider that we would like to create a model that selects a point close to the center. The models on the top row have low bias, meaning the center of the cluster is close to the red dot on the target. The models on the left column have low variance, the clusters are quite tight, meaning our predictions are close together.\n",
    "\n",
    "**Question: What do the blue dots represent? What about the bullseye?**\n",
    "\n",
    "**Question: What is the order of best scenarios?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='the-tradeoff'></a>\n",
    "## The Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to minimize **expected error**, or the average **MSE** over all datasets. It turns out (with some advanced probability gymnastics), that:\n",
    "\n",
    "$$\\text{Mean Squared Error} = \\text{Noise Variance} + \\text{Bias}^2 + \\text{Variance}$$\n",
    "\n",
    "Note that $\\text{Noise Variance}$ is constant: we assume there is some noise, and $\\text{moise variance}$ is simply a value that describes how noisy your dataset will be on average. This is often also called \"irreducible noise\", as it is literally irreducible, we cannot avoid this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, notice that the equation above is the sum of (squared) bias and variance. Thus there is a literal tradeoff between these two, since decreasing one increases the other. This defines what is known as the **bias variance tradeoff**. \n",
    "\n",
    "![alt text](BiasVarianceTradeoff.png \"Bias Variance Tradeoff\")\n",
    "\n",
    "Image from http://scott.fortmann-roe.com/docs/BiasVariance.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does this happen?**\n",
    "\n",
    "At some point, as we decrease **bias**, instead of getting closer to the **true model** $h$, we go past and try to fit to the $\\epsilon$ (noise) that is part of our current dataset. This is equivalent to making our model more noisy, or **overfit** on our dataset, which means that over all datasets, it has more **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions for understanding**:\n",
    "> 1. Where does underfitting and overfitting lie in the graph above? How do they relate to bias and variance?\n",
    "> 2. Why can't we usually just make a bunch of models with low bias and high variance and average them?\n",
    "> 3. Why is low variance important in models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='polynonmial-regression'></a>\n",
    "## Polynomial Regression\n",
    "\n",
    "Let's revisit the polynomial problem that we have discussed.\n",
    "\n",
    "In this case, if our model has degree $d$, we have $d + 1$ features: $x = [x^0, x^1, ..., x^d]$. Now, we have a linear model with $d + 1$ features:\n",
    "\n",
    "$$\\hat{f}(x) = \\sum_{i=0}^{d} a_i x^i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model complexity in this case is the degree of the polynomial. As we saw last week, as $d$ increases, model complexity increases. The model gets better, but then gets erratic. This directly corresponds to the bias-variance graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overfittingDemo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from last time, the best model was a degree 3 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularization'></a>\n",
    "# Regularization\n",
    "\n",
    "We talked about **validation** as a means of combating overfitting. However, this is not the only method to combat overfitting. Another method to do so is to add *regularization* terms to our loss function. **Regularization** basically penalizes complexity in our models. This allows us to add explanatory variables to our model without worrying as much about overfitting. Here's what our ordinary least squares model looks like with a regularization term:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\!\\min_\\theta \\sum_{i=1}^n (y_i - f_\\boldsymbol{\\theta}(x_i))^2 + \\lambda R(\\boldsymbol{\\theta})$$\n",
    "\n",
    "We've written the model a little differently here, but the first term is the same as the ordinary least squares regression model you learned last week. This time it's just generalized to any function of $x$ where $\\theta$ is a list of parameters, or weights on our explanatory variables, such as coefficients to a polynomial. We're minimizing a loss function to find the best coefficients for our model. \n",
    "\n",
    "The second term is the **regularization** term. The $\\lambda$ parameter in front of it dictates how much we care about our regularization term – the higher $\\lambda$ is, the more we penalize large weights, and the more the regularization makes our weights deviate from OLS. \n",
    "\n",
    "**Question**: What happens when $\\lambda = 0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is $R(\\theta)$ in the equation? There are a variety of different regularization functions that could take its place, but today, we'll just talk about the two most common types of functions: **ridge regression** and **LASSO regression**.\n",
    "\n",
    "\n",
    "$$\\begin{align}\\text{ Ridge (L2 Norm)}: &\\  R(\\boldsymbol{\\theta}) = \\|\\theta\\|_2^2 = \\sum_{i=1}^p \\theta_i^2\\\\\n",
    "\\text{ LASSO (L1 Norm)}: &\\ R(\\boldsymbol{\\theta}) = \\|\\theta\\|_1=\\sum_{i=1}^p \\lvert \\theta_i\\rvert\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ridge'></a>\n",
    "## Ridge \n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\!\\min_\\theta \\sum_{i=1}^n (y_i - f_\\boldsymbol{\\theta}(x_i))^2 + \\lambda \\|\\theta\\|_2^2$$\n",
    "In **ridge** regression, the regularization function is the sum of squared weights. One nice thing about ridge regression is that there is always a unique, mathematical solution to minimizing the loss of that term. The solution involves some linear algebra, which we won't get into in this notebook, but the existence of this formula makes this minimization computationally easy to solve!\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$\n",
    "\n",
    "If you recall, the solution to linear regression was of the form:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$\n",
    "\n",
    "And we said that the $\\boldsymbol{X}^T\\boldsymbol{X}$ isn't always invertible. **What about $\\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I}$?**\n",
    "\n",
    "Turns out, this is always invertible! If you are familiar with linear algebra, this is equivalent to adding $\\lambda$ to all the eigenvalues of $X^TX$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To see this in practice**, we'll first create a regular linear regression model, and compare how it does against models using regularization on the `mpg` dataset we used from last week! We'll be constructing models of `displacement` vs. `mpg`, and seeing the difference from there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's construct the `mpg_train` dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpg = pd.read_csv(\"mpg.csv\", index_col='name')# load mpg dataset\n",
    "mpg = mpg.loc[mpg[\"horsepower\"] != '?'].astype(float) # remove columns with missing horsepower values\n",
    "mpg_train, mpg_test = train_test_split(mpg, test_size = .2, random_state = 0) # split into training set and test set\n",
    "mpg_train, mpg_validation = train_test_split(mpg_train, test_size = .5, random_state = 0)\n",
    "mpg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Now, let's create a regular linear regression model using the same process we've learned before (fitting, predicting, finding the loss)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_train = np.vander(mpg_train[\"displacement\"], 13)\n",
    "y_train = mpg_train[[\"mpg\"]]\n",
    "\n",
    "x_validation = np.vander(mpg_validation[\"displacement\"], 13)\n",
    "y_validation = mpg_validation[[\"mpg\"]]\n",
    "\n",
    "# instantiate your model\n",
    "linear_model = ...\n",
    "\n",
    "# fit the model\n",
    "...\n",
    "# make predictions on validation set\n",
    "linear_prediction = ...\n",
    "# find mean squared error\n",
    "linear_loss = ...\n",
    "\n",
    "print(\"Root Mean Squared Error of Linear Model: {:.2f}\".format(linear_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using what you did above as reference, do the same using a Ridge regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "...\n",
    "ridge_loss = ... # mean squared error of ridge model\n",
    "\n",
    "print(\"Root Mean Squared Error of Linear Model: {:.2f}\".format(linear_loss))\n",
    "print(\"Root Mean Squared Error of Ridge Model: {:.2f}\".format(ridge_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lasso'></a>\n",
    "## LASSO\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\!\\min_\\theta \\sum_{i=1}^n (y_i - f_\\boldsymbol{\\theta}(x_i))^2 + \\lambda \\|\\theta\\|_1$$\n",
    "In **LASSO** regression, the regularization function is **the sum of absolute values of the weights**. One key thing to note about **LASSO** is that it is **sparsity inducing**, meaning it forces weights to be zero values rather than really small values (which can happen in **Ridge Regression**), leaving you with fewer explanatory variables in the resulting model! Unlike Ridge Regression, LASSO doesn't necessarily have a unique solution that can be solved for with linear algebra, so there's no formula that determines what the optimal weights should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "...\n",
    "lasso_loss = ... # mean squared error of lasso model\n",
    "\n",
    "print(\"Root Mean Squared Error of Linear Model: {:.2f}\".format(linear_loss))\n",
    "print(\"Root Mean Squared Error of LASSO Model: {:.2f}\".format(lasso_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both **Ridge Regression and LASSO Regression** minimized the loss of our linear regression models, so maybe penalizing some features allowed us to prevent overfitting on our dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualizing-ridge-and-lasso'></a>\n",
    "## Visualizing Ridge and LASSO\n",
    "We went through a lot about **ridge** and **LASSO**, but we didn't really get into what they look like for understanding! And so, here are some visualizations that might help build the intution behind some of the characteristics of these two regularization methods. \n",
    "\n",
    "Another way to describe the modified minimization function above is that it's the same loss function as before, with the *additional constraint* that $R(\\boldsymbol{\\theta}) \\leq t$. Now, $t$ is related to $\\lambda$ but the exact relationship between the two parameters depends on your data. Regardless, let's take a look at what this means in the two-dimensional case. For ridge,\n",
    "\n",
    "$$\\theta_0^2 + \\theta_1^2 = t$$\n",
    "\n",
    "\n",
    "Lasso is of the form $$\\left|\\theta_0\\right| + \\left|\\theta_1\\right| =t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='norm-balls'></a>\n",
    "### Norm Balls\n",
    "\n",
    "Let's take at another visualization that may help build some intuition behind how both of these regularization methods work!\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/L1_and_L2_balls.svg' width=400/>\n",
    "Image from https://upload.wikimedia.org/wikipedia/commons/f/f8/L1_and_L2_balls.svg.\n",
    "<img src='norm_balls.png' width=400/>\n",
    "Image from https://towardsdatascience.com/regression-analysis-lasso-ridge-and-elastic-net-9e65dc61d6d3.\n",
    "\n",
    "The rhombus and circle as a visualization of the regularization term, while the blue circles are the topological curves/level sets representing the loss function based on the weights. You want to minimize the sum of these, which means you want to minimize each of those. The point that minimizes the sum is the minimum point at which they intersect.\n",
    "\n",
    "\n",
    "**Question**: Based on these visualizations, could you explain why LASSO is sparsity-inducing?\n",
    "\n",
    "Turns out that the $L2-norm$ is always some sort of smooth surface, from a circle in 2D to a sphere in 3D. On the other hand, LASSO always has sharp corners. In higher dimensions, it forms an octahedron. This is exactly the feature that makes it sparsiy-inducing. As you might imagine, just as humans are more likely to bump into sharp corners than smooth surfaces, the loss term is also most likely to intersect the $L2-norm$ at one of the corners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularization-and-bias-variance'></a>\n",
    "## Regularization and Bias Variance\n",
    "As we mentioned earlier, **bias** is the average linear squares loss term across multiple models of the same family (e.g. same degree polynomial) trained on separate datasets. **Variance** is the average variance of the weight vectors (coefficients) on your features. \n",
    "\n",
    "Without the regularization term, we’re just minimizing bias; the regularization term means we won’t get the lowest possible bias, but we’re exchanging that for some lower variance so that our model does better at generalizing to data points outside of our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lambda'></a>\n",
    "## Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said that $\\lambda$ is how much we care about the regularization term, but what does that look like? Let's return to the polynomial example from last week, and see what the resulting models look like with different values of $\\lambda$ given a degree 8 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ridgeRegularizationDemo([0, 0.5, 1.0, 5.0, 10.0], 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the diagram above, it's difficult to determine which lambda value help fit our model the closest to the true data distribution. So, **how do we know what to use for $\\lambda$ (or `alpha` in the `sklearn.linear_model` constructors)?**\n",
    "\n",
    "That's right, let's use the process of **validation** here! In this case, we'd be finding the value for lambda that **minimizes the loss for ridge regression, and then the one that minimizes the loss for LASSO regression**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='validation-on-lambda'></a>\n",
    "## Validation on Lambda\n",
    "Let's try to find the best $\\lambda$ for the degree 20 polynomial on `displacement` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.arange(0, 200) # create a list of potential lambda values\n",
    "\n",
    "# create a list containing the corresponding mean_squared_error for each lambda usinb both ridge and lasso regression\n",
    "ridge_errors = [] \n",
    "lasso_errors = []\n",
    "\n",
    "for l in lambdas:\n",
    "    \n",
    "    ridge_errors.append(...)\n",
    "        \n",
    "    lasso_errors.append(...)\n",
    "\n",
    "# finds the index of the minimum value in each list\n",
    "answer = ridge_errors.index(min(ridge_errors)), lasso_errors.index(min(lasso_errors))\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, we've been able to determine which lambdas minimizes our ridge regression model and our LASSO regression model through validation by iterating through potential lambda values and finding the ones that minimize our loss for each model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the course of the notebook, we introduced two main concepts, **bias** and **variance**, and how the two relate with one another when it comes to finding the best model for our dataset! We also went into different methods we can use to minimize overfitting our model, and in turn lower variance, by taking look at a process called **regularization**. We saw that the two main regression models, **ridge regression** and **LASSO regression**, and saw the difference between the two (ridge -> penalize large weights, LASSO -> make weights sparse). We also took a look at different visualizations between the two to build up some more intuition behind how they work, through **graphs** and **norm balls**. Finally, we went through a familiar process (**validation**) to determine what the best values of lambda were for our models, officially ending our journey of **bias** and **variance**, and how we can minimze both in our models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You are now a Bias + Variance master!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "## Exercises\n",
    "\n",
    "1. What happens as $\\lambda$ increases?\n",
    "    1. bias increases, variance increases\n",
    "    2. bias increases, variance decreases\n",
    "    3. bias decreases, variance increases\n",
    "    4. bias decreases, variance decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert answer here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **True** or **False**? Bias is how much error your model makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is **sparsity**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following, choose **ridge**, **lasso**, **both**, or **neither**:\n",
    "    1. L1-norm\n",
    "    2. L2-norm\n",
    "    3. Induces sparsity\n",
    "    4. Has analytic (mathematical) solution\n",
    "    5. Increases bias\n",
    "    6. Increases variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which one is better to use: Ridge Regression or LASSO Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert answer here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats! You've finished our few conceptual questions, now you can help out the rest of your peers and use the rest of the time to work on the intermediate project with your project group!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
